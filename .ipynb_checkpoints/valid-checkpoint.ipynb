{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import scipy.io\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "\n",
    "from darknet import Darknet\n",
    "import dataset\n",
    "from utils import *\n",
    "from MeshPly import MeshPly\n",
    "\n",
    "# Create new directory\n",
    "def makedirs(path):\n",
    "    if not os.path.exists( path ):\n",
    "        os.makedirs( path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "layer     filters    size              input                output\n",
      "    0 conv     32  3 x 3 / 1   416 x 416 x   3   ->   416 x 416 x  32\n",
      "    1 max          2 x 2 / 2   416 x 416 x  32   ->   208 x 208 x  32\n",
      "    2 conv     64  3 x 3 / 1   208 x 208 x  32   ->   208 x 208 x  64\n",
      "    3 max          2 x 2 / 2   208 x 208 x  64   ->   104 x 104 x  64\n",
      "    4 conv    128  3 x 3 / 1   104 x 104 x  64   ->   104 x 104 x 128\n",
      "    5 conv     64  1 x 1 / 1   104 x 104 x 128   ->   104 x 104 x  64\n",
      "    6 conv    128  3 x 3 / 1   104 x 104 x  64   ->   104 x 104 x 128\n",
      "    7 max          2 x 2 / 2   104 x 104 x 128   ->    52 x  52 x 128\n",
      "    8 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "    9 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128\n",
      "   10 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "   11 max          2 x 2 / 2    52 x  52 x 256   ->    26 x  26 x 256\n",
      "   12 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   13 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   14 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   15 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   16 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   17 max          2 x 2 / 2    26 x  26 x 512   ->    13 x  13 x 512\n",
      "   18 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024\n",
      "   19 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512\n",
      "   20 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024\n",
      "   21 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512\n",
      "   22 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024\n",
      "   23 conv   1024  3 x 3 / 1    13 x  13 x1024   ->    13 x  13 x1024\n",
      "   24 conv   1024  3 x 3 / 1    13 x  13 x1024   ->    13 x  13 x1024\n",
      "   25 route  16\n",
      "   26 conv     64  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x  64\n",
      "   27 reorg              / 2    26 x  26 x  64   ->    13 x  13 x 256\n",
      "   28 route  27 24\n",
      "   29 conv   1024  3 x 3 / 1    13 x  13 x1280   ->    13 x  13 x1024\n",
      "   30 conv     20  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x  20\n",
      "   31 detection\n",
      "2022-03-24 10:19:24    Testing ape...\n",
      "2022-03-24 10:19:24    Number of test samples: 1050\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 3), <f4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/GitHub/singleshotpose/venv/lib/python3.8/site-packages/PIL/Image.py:2813\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2812\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2813\u001b[0m     mode, rawmode \u001b[38;5;241m=\u001b[39m \u001b[43m_fromarray_typemap\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtypekey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   2814\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyError\u001b[0m: ((1, 1, 3), '<f4')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 253>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    251\u001b[0m modelcfg   \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcfg/yolo-pose.cfg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    252\u001b[0m weightfile \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackup/ape/model_backup.weights\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 253\u001b[0m \u001b[43mvalid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatacfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweightfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mvalid\u001b[0;34m(datacfg, modelcfg, weightfile)\u001b[0m\n\u001b[1;32m    176\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlim((\u001b[38;5;241m0\u001b[39m, im_width))\n\u001b[1;32m    177\u001b[0m plt\u001b[38;5;241m.\u001b[39mylim((\u001b[38;5;241m0\u001b[39m, im_height))\n\u001b[0;32m--> 178\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(np\u001b[38;5;241m.\u001b[39marray(\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresize(im_height, im_width)))\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# plt.imshow(scipy.misc.imresize(img, (im_height, im_width)))\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# Projections\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m edge \u001b[38;5;129;01min\u001b[39;00m edges_corners:\n",
      "File \u001b[0;32m~/Documents/GitHub/singleshotpose/venv/lib/python3.8/site-packages/PIL/Image.py:2815\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2813\u001b[0m         mode, rawmode \u001b[38;5;241m=\u001b[39m _fromarray_typemap[typekey]\n\u001b[1;32m   2814\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2815\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot handle this data type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m typekey) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   2816\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2817\u001b[0m     rawmode \u001b[38;5;241m=\u001b[39m mode\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 3), <f4"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def valid(datacfg, modelcfg, weightfile):\n",
    "    def truths_length(truths, max_num_gt=50):\n",
    "        for i in range(max_num_gt):\n",
    "            if truths[i][1] == 0:\n",
    "                return i\n",
    "\n",
    "    # Parse configuration files\n",
    "    data_options = read_data_cfg(datacfg)\n",
    "    valid_images = data_options['valid']\n",
    "    meshname     = data_options['mesh']\n",
    "    backupdir    = data_options['backup']\n",
    "    name         = data_options['name']\n",
    "    gpus         = data_options['gpus'] \n",
    "    fx           = float(data_options['fx'])\n",
    "    fy           = float(data_options['fy'])\n",
    "    u0           = float(data_options['u0'])\n",
    "    v0           = float(data_options['v0'])\n",
    "    im_width     = int(data_options['width'])\n",
    "    im_height    = int(data_options['height'])\n",
    "    if not os.path.exists(backupdir):\n",
    "        makedirs(backupdir)\n",
    "\n",
    "    # Parameters\n",
    "    seed = int(time.time())\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = gpus\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    save            = True\n",
    "    visualize       = True\n",
    "    testtime        = True\n",
    "    num_classes     = 1\n",
    "    testing_samples = 0.0\n",
    "    edges_corners = [[0, 1], [0, 2], [0, 4], [1, 3], [1, 5], [2, 3], [2, 6], [3, 7], [4, 5], [4, 6], [5, 7], [6, 7]]\n",
    "    if save:\n",
    "        makedirs(backupdir + '/test')\n",
    "        makedirs(backupdir + '/test/gt')\n",
    "        makedirs(backupdir + '/test/pr')\n",
    "    # To save\n",
    "    testing_error_trans = 0.0\n",
    "    testing_error_angle = 0.0\n",
    "    testing_error_pixel = 0.0\n",
    "    errs_2d             = []\n",
    "    errs_3d             = []\n",
    "    errs_trans          = []\n",
    "    errs_angle          = []\n",
    "    errs_corner2D       = []\n",
    "    preds_trans         = []\n",
    "    preds_rot           = []\n",
    "    preds_corners2D     = []\n",
    "    gts_trans           = []\n",
    "    gts_rot             = []\n",
    "    gts_corners2D       = []\n",
    "\n",
    "    # Read object model information, get 3D bounding box corners\n",
    "    mesh      = MeshPly(meshname)\n",
    "    vertices  = np.c_[np.array(mesh.vertices), np.ones((len(mesh.vertices), 1))].transpose()\n",
    "    corners3D = get_3D_corners(vertices)\n",
    "    try:\n",
    "        diam  = float(options['diam'])\n",
    "    except:\n",
    "        diam  = calc_pts_diameter(np.array(mesh.vertices))\n",
    "        \n",
    "    # Read intrinsic camera parameters\n",
    "    intrinsic_calibration = get_camera_intrinsic(u0, v0, fx, fy)\n",
    "\n",
    "    # Get validation file names\n",
    "    with open(valid_images) as fp:\n",
    "        tmp_files = fp.readlines()\n",
    "        valid_files = [item.rstrip() for item in tmp_files]\n",
    "    \n",
    "    # Specicy model, load pretrained weights, pass to GPU and set the module in evaluation mode\n",
    "    model = Darknet(modelcfg)\n",
    "    model.print_network()\n",
    "    model.load_weights(weightfile)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    test_width    = model.test_width\n",
    "    test_height   = model.test_height\n",
    "    num_keypoints = model.num_keypoints \n",
    "    num_labels    = num_keypoints * 2 + 3\n",
    "\n",
    "    # Get the parser for the test dataset\n",
    "    valid_dataset = dataset.listDataset(valid_images, \n",
    "                                        shape=(test_width, test_height),\n",
    "                                        shuffle=False,\n",
    "                                        transform=transforms.Compose([transforms.ToTensor(),]))\n",
    "\n",
    "    # Specify the number of workers for multiple processing, get the dataloader for the test dataset\n",
    "    kwargs = {'num_workers': 0, 'pin_memory': True}\n",
    "    test_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=False, **kwargs) \n",
    "\n",
    "    logging(\"   Testing {}...\".format(name))\n",
    "    logging(\"   Number of test samples: %d\" % len(test_loader.dataset))\n",
    "    # Iterate through test batches (Batch size for test data is 1)\n",
    "    count = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        \n",
    "        # Images\n",
    "        img = data[0, :, :, :]\n",
    "        img = img.numpy().squeeze()\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        \n",
    "        t1 = time.time()\n",
    "        # Pass data to GPU\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "        # Wrap tensors in Variable class, set volatile=True for inference mode and to use minimal memory during inference\n",
    "        data = Variable(data, volatile=True)\n",
    "        t2 = time.time()\n",
    "        # Forward pass\n",
    "        output = model(data).data  \n",
    "        t3 = time.time()\n",
    "        # Using confidence threshold, eliminate low-confidence predictions\n",
    "        all_boxes = get_region_boxes(output, num_classes, num_keypoints)        \n",
    "        t4 = time.time()\n",
    "        # Evaluation\n",
    "        # Iterate through all batch elements\n",
    "        for box_pr, target in zip([all_boxes], [target[0]]):\n",
    "            # For each image, get all the targets (for multiple object pose estimation, there might be more than 1 target per image)\n",
    "            truths = target.view(-1, num_keypoints*2+3)\n",
    "            # Get how many objects are present in the scene\n",
    "            num_gts    = truths_length(truths)\n",
    "            # Iterate through each ground-truth object\n",
    "            for k in range(num_gts):\n",
    "                box_gt = list()\n",
    "                for j in range(1, 2*num_keypoints+1):\n",
    "                    box_gt.append(truths[k][j])\n",
    "                box_gt.extend([1.0, 1.0])\n",
    "                box_gt.append(truths[k][0])\n",
    "\n",
    "                # Denormalize the corner predictions \n",
    "                # corners2D_gt = np.array(np.reshape(box_gt[:18], [9, 2]), dtype='float32')\n",
    "                # corners2D_pr = np.array(np.reshape(box_pr[:18], [9, 2]), dtype='float32')\n",
    "                box_gt_to_numpy = [item.cpu().data for item in box_gt[:18]]\n",
    "                corners2D_gt = np.array(np.reshape(box_gt_to_numpy, [-1, 2]), dtype='float32')\n",
    "\n",
    "                box_pr_to_numpy = [item.cpu().data for item in box_pr[:18]]\n",
    "                corners2D_pr = np.array(np.reshape(box_pr_to_numpy, [-1, 2]), dtype='float32')\n",
    "                \n",
    "                corners2D_gt[:, 0] = corners2D_gt[:, 0] * im_width\n",
    "                corners2D_gt[:, 1] = corners2D_gt[:, 1] * im_height          \n",
    "                corners2D_pr[:, 0] = corners2D_pr[:, 0] * im_width\n",
    "                corners2D_pr[:, 1] = corners2D_pr[:, 1] * im_height\n",
    "                preds_corners2D.append(corners2D_pr)\n",
    "                gts_corners2D.append(corners2D_gt)\n",
    "\n",
    "                # Compute corner prediction error\n",
    "                corner_norm = np.linalg.norm(corners2D_gt - corners2D_pr, axis=1)\n",
    "                corner_dist = np.mean(corner_norm)\n",
    "                errs_corner2D.append(corner_dist)\n",
    "                \n",
    "                # Compute [R|t] by pnp\n",
    "                R_gt, t_gt = pnp(np.array(np.transpose(np.concatenate((np.zeros((3, 1)), corners3D[:3, :]), axis=1)), dtype='float32'),  corners2D_gt, np.array(intrinsic_calibration, dtype='float32'))\n",
    "                R_pr, t_pr = pnp(np.array(np.transpose(np.concatenate((np.zeros((3, 1)), corners3D[:3, :]), axis=1)), dtype='float32'),  corners2D_pr, np.array(intrinsic_calibration, dtype='float32'))\n",
    "                \n",
    "                # Compute translation error\n",
    "                trans_dist   = np.sqrt(np.sum(np.square(t_gt - t_pr)))\n",
    "                errs_trans.append(trans_dist)\n",
    "                \n",
    "                # Compute angle error\n",
    "                angle_dist   = calcAngularDistance(R_gt, R_pr)\n",
    "                errs_angle.append(angle_dist)\n",
    "                \n",
    "                # Compute pixel error\n",
    "                Rt_gt        = np.concatenate((R_gt, t_gt), axis=1)\n",
    "                Rt_pr        = np.concatenate((R_pr, t_pr), axis=1)\n",
    "                proj_2d_gt   = compute_projection(vertices, Rt_gt, intrinsic_calibration)\n",
    "                proj_2d_pred = compute_projection(vertices, Rt_pr, intrinsic_calibration) \n",
    "                proj_corners_gt = np.transpose(compute_projection(corners3D, Rt_gt, intrinsic_calibration)) \n",
    "                proj_corners_pr = np.transpose(compute_projection(corners3D, Rt_pr, intrinsic_calibration)) \n",
    "                norm         = np.linalg.norm(proj_2d_gt - proj_2d_pred, axis=0)\n",
    "                pixel_dist   = np.mean(norm)\n",
    "                errs_2d.append(pixel_dist)\n",
    "\n",
    "                if visualize:\n",
    "                    # Visualize\n",
    "                    plt.xlim((0, im_width))\n",
    "                    plt.ylim((0, im_height))\n",
    "                    plt.imshow(cv2.resize(img, (im_height, im_width)))\n",
    "                    # plt.imshow(np.array(Image.fromarray(img).resize(im_height, im_width)))\n",
    "                    # plt.imshow(scipy.misc.imresize(img, (im_height, im_width)))\n",
    "                    # Projections\n",
    "                    for edge in edges_corners:\n",
    "                        plt.plot(proj_corners_gt[edge, 0], proj_corners_gt[edge, 1], color='g', linewidth=3.0)\n",
    "                        plt.plot(proj_corners_pr[edge, 0], proj_corners_pr[edge, 1], color='b', linewidth=3.0)\n",
    "                    plt.gca().invert_yaxis()\n",
    "                    plt.show()\n",
    "                \n",
    "                # Compute 3D distances\n",
    "                transform_3d_gt   = compute_transformation(vertices, Rt_gt) \n",
    "                transform_3d_pred = compute_transformation(vertices, Rt_pr)  \n",
    "                norm3d            = np.linalg.norm(transform_3d_gt - transform_3d_pred, axis=0)\n",
    "                vertex_dist       = np.mean(norm3d)    \n",
    "                errs_3d.append(vertex_dist)  \n",
    "\n",
    "                # Sum errors\n",
    "                testing_error_trans  += trans_dist\n",
    "                testing_error_angle  += angle_dist\n",
    "                testing_error_pixel  += pixel_dist\n",
    "                testing_samples      += 1\n",
    "                count = count + 1\n",
    "\n",
    "                if save:\n",
    "                    preds_trans.append(t_pr)\n",
    "                    gts_trans.append(t_gt)\n",
    "                    preds_rot.append(R_pr)\n",
    "                    gts_rot.append(R_gt)\n",
    "\n",
    "                    np.savetxt(backupdir + '/test/gt/R_' + valid_files[count][-8:-3] + 'txt', np.array(R_gt, dtype='float32'))\n",
    "                    np.savetxt(backupdir + '/test/gt/t_' + valid_files[count][-8:-3] + 'txt', np.array(t_gt, dtype='float32'))\n",
    "                    np.savetxt(backupdir + '/test/pr/R_' + valid_files[count][-8:-3] + 'txt', np.array(R_pr, dtype='float32'))\n",
    "                    np.savetxt(backupdir + '/test/pr/t_' + valid_files[count][-8:-3] + 'txt', np.array(t_pr, dtype='float32'))\n",
    "                    np.savetxt(backupdir + '/test/gt/corners_' + valid_files[count][-8:-3] + 'txt', np.array(corners2D_gt, dtype='float32'))\n",
    "                    np.savetxt(backupdir + '/test/pr/corners_' + valid_files[count][-8:-3] + 'txt', np.array(corners2D_pr, dtype='float32'))\n",
    "\n",
    "\n",
    "        t5 = time.time()\n",
    "\n",
    "    # Compute 2D projection error, 6D pose error, 5cm5degree error\n",
    "    px_threshold = 5 # 5 pixel threshold for 2D reprojection error is standard in recent sota 6D object pose estimation works \n",
    "    eps          = 1e-5\n",
    "    acc          = len(np.where(np.array(errs_2d) <= px_threshold)[0]) * 100. / (len(errs_2d)+eps)\n",
    "    acc5cm5deg   = len(np.where((np.array(errs_trans) <= 0.05) & (np.array(errs_angle) <= 5))[0]) * 100. / (len(errs_trans)+eps)\n",
    "    acc3d10      = len(np.where(np.array(errs_3d) <= diam * 0.1)[0]) * 100. / (len(errs_3d)+eps)\n",
    "    acc5cm5deg   = len(np.where((np.array(errs_trans) <= 0.05) & (np.array(errs_angle) <= 5))[0]) * 100. / (len(errs_trans)+eps)\n",
    "    corner_acc   = len(np.where(np.array(errs_corner2D) <= px_threshold)[0]) * 100. / (len(errs_corner2D)+eps)\n",
    "    mean_err_2d  = np.mean(errs_2d)\n",
    "    mean_corner_err_2d = np.mean(errs_corner2D)\n",
    "    nts = float(testing_samples)\n",
    "\n",
    "    if testtime:\n",
    "        print('-----------------------------------')\n",
    "        print('  tensor to cuda : %f' % (t2 - t1))\n",
    "        print('    forward pass : %f' % (t3 - t2))\n",
    "        print('get_region_boxes : %f' % (t4 - t3))\n",
    "        print(' prediction time : %f' % (t4 - t1))\n",
    "        print('            eval : %f' % (t5 - t4))\n",
    "        print('-----------------------------------')\n",
    "\n",
    "    # Print test statistics\n",
    "    logging('Results of {}'.format(name))\n",
    "    logging('   Acc using {} px 2D Projection = {:.2f}%'.format(px_threshold, acc))\n",
    "    logging('   Acc using 10% threshold - {} vx 3D Transformation = {:.2f}%'.format(diam * 0.1, acc3d10))\n",
    "    logging('   Acc using 5 cm 5 degree metric = {:.2f}%'.format(acc5cm5deg))\n",
    "    logging(\"   Mean 2D pixel error is %f, Mean vertex error is %f, mean corner error is %f\" % (mean_err_2d, np.mean(errs_3d), mean_corner_err_2d))\n",
    "    logging('   Translation error: %f m, angle error: %f degree, pixel error: % f pix' % (testing_error_trans/nts, testing_error_angle/nts, testing_error_pixel/nts) )\n",
    "\n",
    "    if save:\n",
    "        predfile = backupdir + '/predictions_linemod_' + name +  '.mat'\n",
    "        scipy.io.savemat(predfile, {'R_gts': gts_rot, 't_gts':gts_trans, 'corner_gts': gts_corners2D, 'R_prs': preds_rot, 't_prs':preds_trans, 'corner_prs': preds_corners2D})\n",
    "\n",
    "datacfg    = 'cfg/ape.data'\n",
    "modelcfg   = 'cfg/yolo-pose.cfg'\n",
    "weightfile = 'backup/ape/model_backup.weights'\n",
    "valid(datacfg, modelcfg, weightfile)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
